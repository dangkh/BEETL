{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Uqb92Uw_CGL1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pVqcC5ixXU_0"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ifVFfEXSCWfQ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path as osp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from beetl.task_datasets import BeetlSleepTutorial, BeetlSleepSource, BeetlSleepLeaderboard, BeetlMILeaderboard\n",
    "# ds = BeetlSleepSource()\n",
    "# path = ds.download()\n",
    "# print(path)\n",
    "# X, y, info = ds.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeetlSleepLeaderboard().download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FL3ZuKQosAOa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=997\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'max_epochs': 20,\n",
    "    'learning_rate': 0.003,\n",
    "    'weight_decay': 0.01,\n",
    "    'scheduler_patience': 5,\n",
    "    'batch_size': 180,\n",
    "    'dropout_conv': 0.1,\n",
    "    'dropout_fc': 0.2,\n",
    "    'early_stop_patience': 20,\n",
    "    'weight_beta': 0.999,\n",
    "    'focal_loss_gamma': 0.5,\n",
    "    'base_filter': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution(y):\n",
    "    (unique, counts) = np.unique(np.asarray(y), return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    plt.bar(unique, counts)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(39):\n",
    "#     _, y, _ = dataset.get_data(subjects=[i])\n",
    "#     plot_label_distribution(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6_OyPmAM0np",
    "outputId": "64e272a9-9544-46a6-fc67-fa3634cae7eb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train, validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data and validation data are from the same set of data and are assumed to be from one distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beetl.task_datasets import BeetlSleepSource\n",
    "dataset = BeetlSleepSource()\n",
    "\n",
    "def get_identity_data(subject):\n",
    "    X, _, _ = dataset.get_data(subjects=[subject])\n",
    "    y = [subject] * X.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, y_train, info = dataset.get_data(subjects=range(0, 35))\n",
    "# X_test, y_test, _ = dataset.get_data(subjects=range(35, 39))\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(0, 39):\n",
    "    X, y = get_identity_data(i)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "Xs = np.concatenate(Xs)\n",
    "ys = np.concatenate(ys)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "IiNTVOfJK5lo",
    "outputId": "3f595d60-c308-4cfe-988a-5dbf8edbdb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72436, 2, 3000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "plt.bar(unique, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_weight = np.power(hyper_params['weight_beta'], np.log(frequencies[:,1]))\n",
    "# loss_weight = (1 - hyper_params['weight_beta']) / (1 - loss_weight)\n",
    "# F.softmax(torch.tensor(loss_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18109, 2, 3000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "(unique, counts) = np.unique(np.asarray(y_test), return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "plt.bar(unique, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, 1, 1)\n",
    "X_test = np.delete(X_test, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above 2 histograms, we can see that they have similar labels' distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f \"^Xs$\"\n",
    "%reset_selective -f \"^ys$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS6FzukMNyby"
   },
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KPNrgRkZN2cE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EEG1ChData(Dataset):\n",
    "    def __init__(self, train_data, train_label, mode='train'):\n",
    "        mean = np.mean(train_data, axis=2, keepdims=True)\n",
    "        std = np.std(train_data, axis=2, keepdims=True)\n",
    "        self.X = (train_data - mean) / std\n",
    "        self.y = train_label\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "#             shape = self.X[0].shape\n",
    "#             noise = np.random.normal(0, 0.05, 6000).reshape(shape)\n",
    "#             X = self.X[idx] + noise\n",
    "            # randomly zero out 10 segments of length 50 to 100\n",
    "            X = self.X[idx]\n",
    "            zeroSegCount = np.random.randint(10)\n",
    "            for i in range(zeroSegCount):\n",
    "                s = np.random.randint(3000)\n",
    "                l = np.random.randint(50, 100)\n",
    "                for j in range(s, min(s + l, 3000)):\n",
    "                    X[0][j] = 0\n",
    "        else:\n",
    "            X = self.X[idx]\n",
    "        if self.y is not None:\n",
    "            return torch.tensor(X, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(X, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_a9V9kTN8fk",
    "outputId": "3b54c2b5-1b7c-4587-acb5-3625cc02c7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72436, 1, 3000)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EEG1ChData(X_train, y_train)\n",
    "test_dataset = EEG1ChData(X_test, y_test, mode='test')\n",
    "print(train_dataset.X.shape) # should be length, 1, 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12.,  29., 149., 660., 844., 872., 349.,  58.,  15.,  12.]),\n",
       " array([-3.83410592, -3.02822537, -2.22234482, -1.41646427, -0.61058373,\n",
       "         0.19529682,  1.00117737,  1.80705792,  2.61293846,  3.41881901,\n",
       "         4.22469956]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_dataset.X[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_a9V9kTN8fk",
    "outputId": "3b54c2b5-1b7c-4587-acb5-3625cc02c7f4"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    sampler=samplers.MPerClassSampler(labels=y_train, m=hyper_params['batch_size'] // 6, \n",
    "                                      length_before_new_iter=hyper_params['batch_size'] * 300), \n",
    "#     shuffle=True,\n",
    "    batch_size=hyper_params['batch_size'],\n",
    "    pin_memory=True,\n",
    "    num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=hyper_params['batch_size'], pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkLPGnDuM1Oo",
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = torch.rand((10, 6), dtype=torch.float)\n",
    "# y = torch.randint(low=0, high=6, size=(10,))\n",
    "# print(y_hat)\n",
    "# print(y)\n",
    "# print(loss_weight)\n",
    "# print(F.cross_entropy(y_hat, y))\n",
    "# print(F.cross_entropy(y_hat, y, torch.tensor(loss_weight, dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from pytorch_metric_learning import losses as loss_fn\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    \n",
    "class BaseNet(pl.LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'train_acc': torch.sum(torch.argmax(y_hat, axis=1) == y, dtype=torch.float32) / len(y)\n",
    "        }\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs[0]]).mean()\n",
    "        avg_acc = torch.stack([x['train_acc'] for x in outputs[0]]).mean()\n",
    "        comet_logs = {'train_loss': avg_loss, 'train_acc': avg_acc}\n",
    "        self.log_dict(comet_logs)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        preds = torch.argmax(y_hat, axis=1)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': torch.sum(preds == y, dtype=torch.float32) / len(y),\n",
    "            'preds': torch.tensor(preds),\n",
    "            'y': torch.tensor(y)\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        comet_logs = {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "        \n",
    "        targets = []\n",
    "        predicted = []\n",
    "        for x in outputs:\n",
    "            targets.extend(x['y'])\n",
    "            predicted.extend(x['preds'])\n",
    "            \n",
    "        targets = F.one_hot(torch.tensor(targets))\n",
    "        predicted = F.one_hot(torch.tensor(predicted))\n",
    "        \n",
    "        experiment = self.logger.experiment\n",
    "        if isinstance(experiment, comet_ml.Experiment):\n",
    "            experiment.log_confusion_matrix(\n",
    "                targets,\n",
    "                predicted,\n",
    "                title=\"Confusion Matrix, Epoch #%d\" % (self.current_epoch + 1),\n",
    "                file_name=\"confusion-matrix-%03d.json\" % (self.current_epoch + 1),\n",
    "            )\n",
    "        self.log_dict(comet_logs)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        return {\n",
    "            'test_loss': loss,\n",
    "            'test_acc': torch.sum(torch.argmax(y_hat, axis=1) == y, dtype=torch.float32) / len(y)\n",
    "        }\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        comet_logs = {'test_loss': avg_loss, 'test_acc': avg_acc}\n",
    "        self.log_dict(comet_logs)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=hyper_params['weight_decay'])\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=hyper_params['scheduler_patience'], min_lr=1e-5),\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, drop_rate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes,\n",
    "                                out_planes,\n",
    "                                i == 0 and stride or 1, drop_rate))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "            out = self.relu2(self.bn2(self.conv1(x)))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "            out = self.relu2(self.bn2(self.conv1(out)))\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GWbJZMGzMyiM",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "    \n",
    "# class LSTMNet(nn.Module):\n",
    "#     def __init__(self, n_classes):\n",
    "#         super(LSTMNet, self).__init__()\n",
    "#         self.drop1 = nn.Dropout(0.5)\n",
    "#         self.drop_ch = nn.Dropout2d(0.1)\n",
    "#         self.fc = nn.Linear(128, n_classes)\n",
    "#         self.n_classes = n_classes\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=[10, 2], stride=1)\n",
    "#         self.bn1 = nn.BatchNorm1d(16)\n",
    "        \n",
    "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=10, stride=1)\n",
    "#         self.bn2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "#         self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=10, stride=1)\n",
    "#         self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "#         self.pool = nn.MaxPool1d(4)\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "\n",
    "#         torch.nn.init.kaiming_normal_(self.conv1.weight)\n",
    "#         torch.nn.init.kaiming_normal_(self.conv2.weight)\n",
    "#         torch.nn.init.kaiming_normal_(self.conv3.weight)\n",
    "#         torch.nn.init.kaiming_normal_(self.fc.weight)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = self.bn1(self.pool(F.relu(self.conv1(x)).reshape(x.shape[0], self.conv1.out_channels, -1)))\n",
    "#         output = torch.squeeze(self.drop_ch(torch.unsqueeze(output, 3)))\n",
    "        \n",
    "#         output = self.bn2(self.pool(F.relu(self.conv2(output))))\n",
    "#         output = torch.squeeze(self.drop_ch(torch.unsqueeze(output, 3)))\n",
    "        \n",
    "#         output = self.bn3(self.pool(F.relu(self.conv3(output))))\n",
    "#         output = torch.squeeze(self.drop_ch(torch.unsqueeze(output, 3)))\n",
    "#         # output's shape is batch, 64, 37\n",
    "        \n",
    "#         outputs, _ = self.lstm(torch.transpose(output, 2, 1)) # batch, seq_len, hidden_size\n",
    "# #         _, (outputs, _) = self.lstm(torch.transpose(output, 2, 1)) # batch, seq_len, hidden_size\n",
    "#         output = torch.mean(outputs, 1) # batch, hidden_size\n",
    "# #         output = torch.squeeze(outputs)\n",
    "#         output = F.relu(self.fc(self.drop1(output)))\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses as metric_losses\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "class WvNet(BaseNet):\n",
    "    def __init__(self, learning_rate, base_filter, drop_conv, drop_fc):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.drop_fc = nn.Dropout(drop_fc)\n",
    "        self.drop_conv = nn.Dropout2d(drop_conv)\n",
    "        self.n_classes = 39\n",
    "        self.conv_f_size = 32\n",
    "        self.base_filter = base_filter\n",
    "        self.filter_size = [self.base_filter * (i+1) for i in range(16)]\n",
    "        \n",
    "        self.conv_fs = nn.ModuleList()\n",
    "        for i in range(len(self.filter_size)):\n",
    "            self.conv_fs.append(\n",
    "                    nn.Sequential(OrderedDict([\n",
    "                        ('conv', nn.Conv1d(in_channels=1, out_channels=self.conv_f_size, kernel_size=[self.filter_size[i]], stride=1, bias=False)),\n",
    "                        ('act', nn.ReLU()),\n",
    "                        ('pool', nn.AdaptiveMaxPool1d((128))),\n",
    "                        ('bn', nn.BatchNorm1d(self.conv_f_size))\n",
    "                    ])\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.pool_conv_1 = nn.AvgPool2d((2, 2))\n",
    "        self.conv_gr1 = nn.Conv2d(in_channels=len(self.conv_fs), out_channels=16, kernel_size=[3, 7], stride=1, padding=(1, 3), groups=1)\n",
    "        self.bn_gr1 = nn.BatchNorm2d(self.conv_gr1.out_channels)\n",
    "\n",
    "        self.pool_conv_2 = nn.AvgPool2d((2, 2))\n",
    "        self.conv_gr2 = nn.Conv2d(in_channels=self.conv_gr1.out_channels, out_channels=48, kernel_size=[3, 7], stride=1, padding=(1, 3), groups=1)\n",
    "        self.bn_gr2 = nn.BatchNorm2d(self.conv_gr2.out_channels)\n",
    "        \n",
    "        self.conv_gr3 = nn.Conv2d(in_channels=self.conv_gr2.out_channels, out_channels=128, kernel_size=[8, 8], stride=1, groups=2)\n",
    "        self.bn_gr3 = nn.BatchNorm2d(self.conv_gr3.out_channels)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.conv_gr3.out_channels, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.loss_fn = metric_losses.ArcFaceLoss(num_classes=self.n_classes, embedding_size=self.lstm.hidden_size)\n",
    "        \n",
    "        for conv_f in self.conv_fs:\n",
    "            torch.nn.init.kaiming_normal_(conv_f.conv.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_gr1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_gr2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_gr3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(len(self.conv_fs)):\n",
    "            output = self.conv_fs[i](x)\n",
    "            output = self.drop_conv(output)\n",
    "            output = torch.squeeze(output)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = torch.stack((outputs), dim=1)\n",
    "        \n",
    "        output = self.bn_gr1(self.pool_conv_1(F.relu(self.conv_gr1(output))))\n",
    "        output = self.drop_conv(output)\n",
    "        \n",
    "        output = self.bn_gr2(self.pool_conv_2(F.relu(self.conv_gr2(output))))\n",
    "        output = self.drop_conv(output)\n",
    "        \n",
    "        output = self.bn_gr3(F.relu(self.conv_gr3(output)))\n",
    "        output = torch.squeeze(output)\n",
    "        \n",
    "        # simple fc for output\n",
    "        _, (outputs, _) = self.lstm(torch.transpose(output, 2, 1)) # batch, seq_len, hidden_size\n",
    "        output = outputs[1] # get hidden of backward pass\n",
    "\n",
    "        return output\n",
    "    \n",
    "    # don't care yet about optimizer_idx\n",
    "    # 0 - main optimizer\n",
    "    # 1 - arcface loss optimizer\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        loss = self.loss_fn(embeddings, y)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "        }\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs[0]]).mean()\n",
    "        \n",
    "        comet_logs = {'train_loss': avg_loss}\n",
    "        self.log_dict(comet_logs)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        loss = self.loss_fn(embeddings, y)\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'embeddings': embeddings,\n",
    "            'y': y\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        \n",
    "        embeddings = torch.cat([x['embeddings'] for x in outputs], dim=0)\n",
    "        y = torch.cat([x['y'] for x in outputs], dim=0)\n",
    "        acc_calculator = AccuracyCalculator(exclude=('r_precision', 'mean_average_precision_at_r'), k=100)\n",
    "        accs = acc_calculator.get_accuracy( \n",
    "            query=embeddings,      \n",
    "            reference=embeddings, \n",
    "            query_labels=y, \n",
    "            reference_labels=y, \n",
    "            embeddings_come_from_same_source=True, \n",
    "        )\n",
    "        comet_logs = {\n",
    "            'val_loss': avg_loss,\n",
    "            'AMI': accs['AMI'],\n",
    "            'NMI': accs['NMI'],\n",
    "            'precision_at_1': accs['precision_at_1']\n",
    "        }\n",
    "        self.log_dict(comet_logs)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        embeddings = self(x)\n",
    "        loss = self.loss_fn(embeddings, y)\n",
    "        return {\n",
    "            'test_loss': loss\n",
    "        }\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        comet_logs = {'test_loss': avg_loss}\n",
    "        self.log_dict(comet_logs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=hyper_params['weight_decay'])\n",
    "        scheduler = {\n",
    "            'scheduler': lr_scheduler.OneCycleLR(\n",
    "                optimizer, \n",
    "                max_lr=hyper_params['learning_rate'], \n",
    "                steps_per_epoch=300, \n",
    "                epochs=hyper_params['max_epochs'], \n",
    "                pct_start=0.3, \n",
    "                div_factor=3,\n",
    "                final_div_factor=1,\n",
    "                three_phase=True\n",
    "            ),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        \n",
    "        loss_optimizer = torch.optim.Adam(self.loss_fn.parameters(), lr=0.001, weight_decay=hyper_params['weight_decay'])\n",
    "        return [optimizer, loss_optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WvConvNet(BaseNet):\n",
    "    def __init__(self, learning_rate, depth, widen_factor=1, drop_rate=0.5, flatten=True, stride=1):\n",
    "        super(WvConvNet, self).__init__()\n",
    "        self.drop_fc = nn.Dropout(0.1)\n",
    "        self.drop_conv = nn.Dropout2d(0.1)\n",
    "        self.n_classes = 6\n",
    "        self.learning_rate = learning_rate\n",
    "        self.conv_f_size = 32\n",
    "        \n",
    "        self.conv_fs = nn.ModuleList()\n",
    "        self.bn_fs = nn.ModuleList()\n",
    "        self.pool_fs = nn.ModuleList()\n",
    "        for i in range(16):\n",
    "            self.conv_fs.append(nn.Conv1d(in_channels=1, out_channels=self.conv_f_size, kernel_size=[5], stride=1, dilation=(2*(i+1))))\n",
    "            self.bn_fs.append(nn.BatchNorm1d(self.conv_f_size))\n",
    "            self.pool_fs.append(nn.AdaptiveAvgPool1d(32))\n",
    "\n",
    "        n_channels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        #self.conv1 = nn.Conv2d(4, n_channels[0], kernel_size=3, stride=1,\n",
    "        #                       padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, stride, drop_rate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2, drop_rate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2, drop_rate)\n",
    "\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.nChannels = n_channels[3]\n",
    "\n",
    "        self.linear = nn.Linear(640, self.n_classes)\n",
    "\n",
    "        if flatten:\n",
    "            self.final_feat_dim = 640\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "        for conv in self.conv_fs:\n",
    "            torch.nn.init.kaiming_normal_(conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(len(self.conv_fs)):\n",
    "            output = F.relu(self.conv_fs[i](x))\n",
    "            output = self.bn_fs[i](self.pool_fs[i](output))\n",
    "#             output = self.drop_conv(output)\n",
    "            output = torch.squeeze(output)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = torch.stack((outputs), dim=1)\n",
    "#         output = self.drop_conv(output)\n",
    "        \n",
    "        # print(output.shape)\n",
    "\n",
    "        #out = self.conv1(output)\n",
    "        out = self.block1(output)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(x.size(0), -1)\n",
    "\n",
    "        logits = F.relu(self.linear(self.drop_fc(out)))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'train_acc': torch.sum(torch.argmax(y_hat, axis=1) == y, dtype=torch.float32) / len(y)\n",
    "        }\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=hyper_params['weight_decay'])\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler.OneCycleLR(optimizer, max_lr=hyper_params['learning_rate'], steps_per_epoch=300, epochs=hyper_params['max_epochs'], pct_start=0.3, div_factor=3),\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = WvNet(\n",
    "    learning_rate=hyper_params['learning_rate'], \n",
    "    base_filter=hyper_params['base_filter'],\n",
    "    drop_conv=hyper_params['dropout_conv'], \n",
    "    drop_fc=hyper_params['dropout_fc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(torch.rand(2, 1, 3000))\n",
    "assert(out.shape == (2, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "/home/peara/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory ../models/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "comet_config = {\n",
    "    'api_key': 'WRx11j64HCTDOp2MpXBExSTdI',\n",
    "    'project_name': 'eeg-identity',\n",
    "    'workspace': 'peara',\n",
    "}\n",
    "\n",
    "\n",
    "# arguments made to CometLogger are passed on to the comet_ml.Experiment class\n",
    "comet_logger = CometLogger(\n",
    "    api_key=comet_config['api_key'],\n",
    "    workspace=comet_config['workspace'],\n",
    "    project_name=comet_config['project_name'],\n",
    "    experiment_name=f'WvNet 16 channel + {hyper_params[\"base_filter\"]} * i + 32 features + arcface + silu'\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=hyper_params['early_stop_patience'],\n",
    "    min_delta=0.0005,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../models/',\n",
    "    filename='wvnet-1ch-f3-{epoch}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hyper_params['max_epochs'], logger=comet_logger, deterministic=True, callbacks=[lr_monitor, early_stop_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/peara/eeg-identity/5bd2f6e772f34458a2dc6e50d0a33c8e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.logger.log_hyperparams(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "COMET WARNING: Empty mapping given to log_params({}); ignoring\n",
      "\n",
      "   | Name        | Type        | Params\n",
      "---------------------------------------------\n",
      "0  | drop_fc     | Dropout     | 0     \n",
      "1  | drop_conv   | Dropout2d   | 0     \n",
      "2  | conv_fs     | ModuleList  | 22.8 K\n",
      "3  | pool_conv_1 | AvgPool2d   | 0     \n",
      "4  | conv_gr1    | Conv2d      | 5.4 K \n",
      "5  | bn_gr1      | BatchNorm2d | 32    \n",
      "6  | pool_conv_2 | AvgPool2d   | 0     \n",
      "7  | conv_gr2    | Conv2d      | 16.2 K\n",
      "8  | bn_gr2      | BatchNorm2d | 96    \n",
      "9  | conv_gr3    | Conv2d      | 196 K \n",
      "10 | bn_gr3      | BatchNorm2d | 256   \n",
      "11 | lstm        | LSTM        | 264 K \n",
      "12 | loss_fn     | ArcFaceLoss | 5.0 K \n",
      "---------------------------------------------\n",
      "510 K     Trainable params\n",
      "0         Non-trainable params\n",
      "510 K     Total params\n",
      "2.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peara/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 997\n",
      "/home/peara/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f7afe0c3f14ebd948b1e71ec964ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset_selective -f \"^train_loader$\"\n",
    "%reset_selective -f \"^train_dataset$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f \"^test_loader$\"\n",
    "%reset_selective -f \"^test_dataset$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Test data is from a different set of subjects and have a different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from beetl.task_datasets import BeetlSleepLeaderboard\n",
    "X_sleep_target, y_sleep_target, _, _ = BeetlSleepLeaderboard().get_data(subjects=range(0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15442, 1, 3000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sleep_target = np.delete(X_sleep_target, 1, 1)\n",
    "print(X_sleep_target.shape)\n",
    "(unique, counts) = np.unique(y_sleep_target, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ynecnk8ab1my"
   },
   "outputs": [],
   "source": [
    "transfer_dataset = EEG1ChData(X_sleep_target, y_sleep_target, mode='test')\n",
    "transfer_loader = torch.utils.data.DataLoader(dataset=transfer_dataset, batch_size=hyper_params['batch_size'], pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_weight = [0.1] * 6\n",
    "# model = WvNet.load_from_checkpoint('../models/wvnet-1ch-epoch=25-val_loss=0.35.ckpt', learning_rate=0.01, drop_conv=0.1, drop_fc=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-fe674359f58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# plugin will setup fitting (e.g. ddp will launch child processes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mpre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# save exp to get started (this is where the first experiment logs are written)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/comet.py\u001b[0m in \u001b[0;36mlog_hyperparams\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDummyExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mget_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDummyExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/comet.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     self._experiment = CometExistingExperiment(\n\u001b[0m\u001b[1;32m    215\u001b[0m                         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                         \u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, previous_experiment, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 )\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExistingExperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_experiment_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level, optimizer_data, auto_weight_logging, auto_log_co2, auto_metric_step_rate, auto_histogram_tensorboard_logging, auto_histogram_epoch_rate, auto_histogram_weight_logging, auto_histogram_gradient_logging, auto_histogram_activation_logging)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malive\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/experiment.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_streamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m_setup_streamer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Initiate the streamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_streamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ws_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/__init__.py\u001b[0m in \u001b[0;36m_initialize_streamer\u001b[0;34m(self, full_ws_url, initial_offset)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebSocketConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ws_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         self.streamer = Streamer(\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws_connection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/comet_ml/connection.py\u001b[0m in \u001b[0;36mwait_for_connection\u001b[0;34m(self, num_of_tries)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_of_tries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.test(model, transfer_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f \"^transfer_loader$\"\n",
    "%reset_selective -f \"^transfer_dataset$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model.conv_fs[1].conv.weight[0][0].cpu().detach().numpy()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "t, y = signal.impulse(([1.0], f))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"../models/wvnet.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsNlUvv9ck56"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beetl.task_datasets import BeetlSleepLeaderboard\n",
    "_, _, X_sleep_test, _ = BeetlSleepLeaderboard().get_data(subjects=range(6, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sleep_test = np.delete(X_sleep_test, 1, 1)\n",
    "submission_dataset = EEG1ChData(X_sleep_test, None, mode='test')\n",
    "submission_loader = torch.utils.data.DataLoader(dataset=submission_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WvNet.load_from_checkpoint(checkpoint_callback.best_model_path, learning_rate=0.01, drop_conv=0, drop_fc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = trainer.predict(model, submission_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "for predicts in predicts:\n",
    "    predicted_labels.extend(np.argmax(predicts.cpu().detach().numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_labels[:20])\n",
    "np.savetxt(\"answer.txt\",predicted_labels,delimiter=',',fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_distribution(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NIP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
